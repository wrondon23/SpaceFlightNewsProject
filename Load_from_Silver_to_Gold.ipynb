{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# Load_from_Silver_to_Gold\n##### En la capa Gold, los datos suelen estar optimizados para análisis específicos, modelos de negocio o dashboards, presentándose en una forma altamente agregada, segmentada o modelada según los requisitos del usuario final. Este paso suele incluir cálculos avanzados, agregaciones y enriquecimiento de información.\n\n\n\n\n\n\n\n.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom awsglue.context import GlueContext\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType\nimport re\nfrom awsglue.job import Job\nfrom pyspark.context import SparkContext\nimport boto3\n\n# Obtener los argumentos del trabajo\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\n\n# Crear contexto de Glue\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\n\n# Crear el objeto Job\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nSession ID: 973d96b3-247e-4b9e-a5d2-e4477f81d39b\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session 973d96b3-247e-4b9e-a5d2-e4477f81d39b to get into ready status...\nSession 973d96b3-247e-4b9e-a5d2-e4477f81d39b has been created.\nValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=GlueReplApp, master=jes) created by __init__ at /tmp/32870217093724463:514 \n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Ruta del archivo Parquet en S3\n\nparquet_path = \"s3://spaceflightbuckes2025/Silver /cleaned_data/part-*.parquet\"\n# Lectura del archivo Parquet con esquema inferido\ndata_cleaned = spark.read.parquet(parquet_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# -------------------------------\n# Paso 1: Análisis de Contenido\n# -------------------------------\n\n# Lista de stopwords básicas (puedes ampliar esta lista si lo deseas)\nstop_words = set([\n    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \n    \"are\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \n    \"between\", \"both\", \"but\", \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\", \"can't\", \n    \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"don't\", \"doing\", \"don't\", \"down\", \n    \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\", \"has\", \"hasn't\", \n    \"haven't\", \"having\", \"he\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"he's\", \n    \"how\", \"how's\", \"however\", \"i\", \"i'm\", \"i've\", \"i'll\", \"i'd\", \"i'll\", \"i'm\", \"if\", \"is\", \n    \"isn't\", \"it's\", \"let\", \"me\", \"more\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"on\", \n    \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \n    \"own\", \"same\", \"she\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \"such\", \"than\", \n    \"that\", \"that's\", \"that'll\", \"that'd\", \"that's\", \"the\", \"that's\", \"theirs\", \"them\", \n    \"they\", \"they're\", \"they've\", \"they'd\", \"this\", \"this's\", \"those\", \"through\", \"to\", \n    \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we're\", \"we've\", \n    \"we'll\", \"we'd\", \"we're\", \"what\", \"what's\", \"what's\", \"when\", \"when's\", \"when\", \n    \"where\", \"where's\", \"where\", \"while\", \"who\", \"who's\", \"whom\", \"who's\", \"why\", \"why's\"\n])\n\n# Función para extraer palabras clave\ndef extract_keywords(text):\n    if not text:\n        return None\n    text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())  # Limpiar texto\n    tokens = text.split()  # Tokenización básica\n    keywords = [word for word in tokens if word not in stop_words]  # Filtrar stopwords\n    return \",\".join(keywords)\n\n# UDF para Spark\nextract_keywords_udf = udf(extract_keywords, StringType())\n\n# Aplicar UDF para palabras clave\ncleaned_data = data_cleaned.withColumn(\"keywords\", extract_keywords_udf(col(\"summary\")))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# -------------------------------\n# Paso 2: Análisis de Sentimiento usando AWS Comprehend\n# -------------------------------\n\nimport boto3\nfrom pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import IntegerType\n\n# Función para obtener análisis de sentimiento y agregar la puntuación numérica\ndef detect_sentiment(text):\n    if text:\n        # Crear el cliente de AWS Comprehend dentro de la función\n        comprehend_client = boto3.client('comprehend', region_name='us-east-1')  # Ajusta la región si es necesario\n        \n        response = comprehend_client.batch_detect_sentiment(\n            TextList=[text],\n            LanguageCode='en'  # O 'es' si los textos están en español\n        )\n        \n        sentiment = response['ResultList'][0]['Sentiment']\n        \n        # Asignar puntuación numérica basada en el sentimiento\n        if sentiment == \"POSITIVE\":\n            score = 1\n        elif sentiment == \"NEGATIVE\":\n            score = -1\n        elif sentiment == \"NEUTRAL\":\n            score = 0\n        else:\n            score = 2  # Para el sentimiento MIXED, asignamos 2 (puedes cambiarlo si lo prefieres)\n        \n        return score\n    \n    return None\n\n# Registrar UDF para análisis de sentimiento\ndetect_sentiment_udf = udf(detect_sentiment, IntegerType())\n\n# Aplicar análisis de sentimiento y agregar la columna con puntuación numérica\ncleaned_data = cleaned_data.withColumn(\"sentiment_score\", detect_sentiment_udf(col(\"summary\")))\n          \n\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SQLContext\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import StringType\n# -------------------------------\n# Paso 2:Modelo de clasificacion\n# -------------------------------\n\nclassification_dict = {\n    # Términos astronómicos\n    \"Space\": \"Astronomical terminology\",\n    \"Galaxy\": \"Astronomical terminology\",\n    \"Universe\": \"Astronomical terminology\",\n    \"Orbit\": \"Astronomical terminology\",\n    \"Star\": \"Astronomical terminology\",\n    \"Constellation\": \"Astronomical terminology\",\n    \"Planet\": \"Astronomical terminology\",\n    \"Satellite\": \"Astronomical terminology, Technology and science\",\n    \"Asteroid\": \"Astronomical terminology\",\n    \"Comet\": \"Astronomical terminology\",\n    \"Black hole\": \"Astronomical terminology\",\n    \"Nebula\": \"Astronomical terminology\",\n    \"Meteorite\": \"Astronomical terminology\",\n    \"Gravity\": \"Astronomical terminology, Physics\",\n    \"Microgravity\": \"Astronomical terminology, Physics\",\n    \"Exoplanet\": \"Astronomical terminology\",\n    \n    # Términos relacionados con vehículos espaciales\n    \"Rocket\": \"Spacecraft and vehicles\",\n    \"Shuttle\": \"Spacecraft and vehicles\",\n    \"Spaceship\": \"Spacecraft and vehicles\",\n    \"Probe\": \"Spacecraft and vehicles\",\n    \"Space station\": \"Spacecraft and vehicles\",\n    \"Lunar module\": \"Spacecraft and vehicles\",\n    \"Rover\": \"Spacecraft and vehicles\",\n    \"Thruster\": \"Spacecraft and vehicles\",\n    \"Vehicle\": \"Spacecraft and vehicles\",\n    \"Capsule\": \"Spacecraft and vehicles\",\n    \"Falcon\": \"Spacecraft and vehicles\",\n    \"Artemis\": \"Spacecraft and vehicles\",\n    \"Orion\": \"Spacecraft and vehicles\",\n    \n    # Términos de misiones y exploración\n    \"Exploration\": \"Missions and exploration\",\n    \"Colonization\": \"Missions and exploration\",\n    \"Mission\": \"Missions and exploration\",\n    \"Liftoff\": \"Missions and exploration\",\n    \"Landing\": \"Missions and exploration\",\n    \"Launch\": \"Missions and exploration\",\n    \"Docking\": \"Missions and exploration\",\n    \"Mars\": \"Missions and exploration\",\n    \"Moon\": \"Missions and exploration\",\n    \"Journey\": \"Missions and exploration\",\n    \"Discovery\": \"Missions and exploration\",\n    \"Research\": \"Missions and exploration\",\n    \n    # Términos relacionados con tecnología y ciencia\n    \"Propulsion\": \"Technology and science\",\n    \"Fuel\": \"Technology and science\",\n    \"Solar panel\": \"Technology and science\",\n    \"Artificial intelligence\": \"Technology and science\",\n    \"Robotics\": \"Technology and science\",\n    \"Innovation\": \"Technology and science\",\n    \"Engineering\": \"Technology and science\",\n    \"Physics\": \"Technology and science\",\n    \"Astronomy\": \"Technology and science\",\n    \"Science\": \"Technology and science\",\n    \"Instruments\": \"Technology and science\",\n    \"Telescope\": \"Technology and science\",\n    \"Sensors\": \"Technology and science\",\n    \"Simulation\": \"Technology and science\",\n    \n    # Organizaciones y programas espaciales\n    \"NASA\": \"Organizations and space programs\",\n    \"ESA (European Space Agency)\": \"Organizations and space programs\",\n    \"SpaceX\": \"Organizations and space programs\",\n    \"Blue Origin\": \"Organizations and space programs\",\n    \"Roscosmos\": \"Organizations and space programs\",\n    \"ISRO (Indian Space Research Organization)\": \"Organizations and space programs\",\n    \"CNSA (China National Space Administration)\": \"Organizations and space programs\",\n    \"JAXA (Japan Aerospace Exploration Agency)\": \"Organizations and space programs\",\n    \"Virgin Galactic\": \"Organizations and space programs\",\n    \n    # Personas y profesiones\n    \"Astronaut\": \"People and professions\",\n    \"Cosmonaut\": \"People and professions\",\n    \"Scientist\": \"People and professions\",\n    \"Engineer\": \"People and professions\",\n    \"Pilot\": \"People and professions\",\n    \"Explorer\": \"People and professions\",\n    \"Researcher\": \"People and professions\",\n    \"Visionary\": \"People and professions\",\n    \n    # Conceptos futuristas y de ciencia ficción\n    \"Martian colonization\": \"Futuristic concepts and sci-fi\",\n    \"Extraterrestrial life\": \"Futuristic concepts and sci-fi\",\n    \"Terraforming\": \"Futuristic concepts and sci-fi\",\n    \"Hyperspace\": \"Futuristic concepts and sci-fi\",\n    \"Interstellar travel\": \"Futuristic concepts and sci-fi\",\n    \"Space tourism\": \"Futuristic concepts and sci-fi\",\n    \"Space habitats\": \"Futuristic concepts and sci-fi\",\n    \"Warp drive\": \"Futuristic concepts and sci-fi\",\n    \"Space elevator\": \"Futuristic concepts and sci-fi\",\n    \"Wormhole\": \"Futuristic concepts and sci-fi\",  # Añadido nuevo término\n    \n    # Nuevos términos adicionales\n    \"Quantum mechanics\": \"Technology and science\",  # Añadido nuevo término\n    \"Artificial gravity\": \"Technology and science\",  # Añadido nuevo término\n}\n\n\n# Función para clasificar el título\ndef classify_title(title):\n    for keyword, category in classification_dict.items():\n        if keyword.lower() in title.lower():  # Compara en minúsculas para no diferenciar por mayúsculas\n            return category\n    return \"Unclassified\"  # Si no se encuentra ninguna palabra clave\n\n# Convertimos la función en una UDF (User Defined Function)\nclassify_udf = udf(classify_title, StringType())\n\n# Inicialización de Spark\nsqlContext = SQLContext(sc)\n\nFinal_data = cleaned_data.withColumn(\"classification\", classify_udf(col(\"title\")))\n\n# Aplicamos la clasificación\n#data_with_TopicClasificacion = data_with_TopicClasificacion.withColumn(\"classification\", classify_udf(col(\"title\")))\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Guardar los resultados de source_analysis en S3\nsource_analysis_output_path = \"s3://spaceflightbuckes2025/Gold /Final_Data/\"\nFinal_data.write.mode(\"overwrite\").parquet(source_analysis_output_path)\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Función para extraer palabras clave\ndef extract_keywords(text):\n    if not text:\n        return None\n    text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())  # Limpiar texto\n    tokens = text.split()  # Tokenización básica\n    keywords = [word for word in tokens if word not in stop_words]  # Filtrar stopwords\n    return \",\".join(keywords)\n\n# UDF para Spark\nextract_keywords_udf = udf(extract_keywords, StringType())\n\n# Aplicar UDF para palabras clave\nanalyzed_data = Final_data.withColumn(\"keywords\", extract_keywords_udf(col(\"summary\")))\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# -------------------------------\n# Análisis de Tendencias\n# -------------------------------\n\n# Conteo de artículos por fuente de noticias\nsource_analysis = analyzed_data.groupBy(\"news_site\") \\\n    .agg(count(\"*\").alias(\"article_count\")) \\\n    .orderBy(desc(\"article_count\"))\n\n# Guardar los resultados de source_analysis en S3\nsource_analysis_output_path = \"s3://spaceflightbuckes2025/Gold /CountArticulesNews/\"\nsource_analysis.write.mode(\"overwrite\").parquet(source_analysis_output_path)\n\n# Tendencias de temas por tiempo\ntrends_analysis = analyzed_data.groupBy(\"published_at\") \\\n    .agg(collect_list(\"keywords\").alias(\"keywords_list\"))\n\n# Guardar los resultados de trends_analysis en S3\ntrends_analysis_output_path = \"s3://spaceflightbuckes2025/Gold /trends_analysis/\"\ntrends_analysis.write.mode(\"overwrite\").parquet(trends_analysis_output_path)\n\n# -------------------------------\n# Optimizaciones Requeridas\n# -------------------------------\n\n# Particionar datos históricos por año y mes\npartitioned_data = analyzed_data.withColumn(\"year\", year(\"published_at\")) \\\n    .withColumn(\"month\", month(\"published_at\"))\n\n# Escribir datos particionados en S3\noutput_path = \"s3://spaceflightbuckes2025/Gold /partitioned_data/\"\npartitioned_data.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").parquet(output_path)\n\n# -------------------------------\n# Caching de resultados frecuentes\n# -------------------------------\n\n# Cache de resultados más consultados (ejemplo: fuentes activas)\nsource_analysis.cache()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[news_site: string, article_count: bigint]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "job.commit()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "NameError: name 'job' is not defined\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}