{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# Transform_from_Bronze_to_Silver\n##### es un proceso en la arquitectura de datos que implica la transformación y limpieza de datos desde una capa inicial de almacenamiento (Bronze) hacia una capa más refinada y estructurada (Silver).\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom awsglue.context import GlueContext\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType\nimport re\nfrom awsglue.job import Job\nfrom pyspark.context import SparkContext\nimport boto3\n\n# Obtener los argumentos del trabajo\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\n\n# Crear contexto de Glue\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\n\n# Crear el objeto Job\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nSession ID: 97cbb2b7-60ab-4ec5-84d1-90fa7011284e\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session 97cbb2b7-60ab-4ec5-84d1-90fa7011284e to get into ready status...\nSession 97cbb2b7-60ab-4ec5-84d1-90fa7011284e has been created.\nGlueArgumentError: the following arguments are required: --JOB_NAME\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\n\n# Definir el esquema basado en el objeto JSON\nschema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"url\", StringType(), True),\n    StructField(\"image_url\", StringType(), True),\n    StructField(\"news_site\", StringType(), True),\n    StructField(\"summary\", StringType(), True),\n    StructField(\"published_at\", TimestampType(), True),\n    StructField(\"updated_at\", TimestampType(), True)\n])\n\n# Rutas de los archivos CSV en S3\ncsv_path1 = \"s3://spaceflightbuckes2025/Bronze/data_articules.csv\"\ncsv_path2 = \"s3://spaceflightbuckes2025/Bronze/data_blogs.csv\"\ncsv_path3 = \"s3://spaceflightbuckes2025/Bronze/data_reports.csv\"\n\n\n# Leer los archivos CSV con el esquema definido\ndf1 = spark.read.option(\"header\", True).schema(schema).csv(csv_path1)\ndf2 = spark.read.option(\"header\", True).schema(schema).csv(csv_path2)\ndf3 = spark.read.option(\"header\", True).schema(schema).csv(csv_path3)\n\ndf1_final = df1.filter(df1[\"id\"].isNotNull())\n\n# Identificar todas las columnas únicas\nall_columns = set(df1.columns).union(df2.columns).union(df3.columns)\n\n# Función para alinear columnas faltantes\ndef align_columns(df, all_columns):\n    for col in all_columns:\n        if col not in df.columns:\n            df = df.withColumn(col, lit(None))  # Agregar columna con valores nulos\n    return df.select(sorted(all_columns))  # Ordenar columnas alfabéticamente\n\n# Alinear las columnas de los DataFrames\ndf1_aligned = align_columns(df1_final, all_columns)\ndf2_aligned = align_columns(df2, all_columns)\ndf3_aligned = align_columns(df3, all_columns)\n\n# Unir los DataFrames\ndata = df1_aligned.unionByName(df2_aligned).unionByName(df3_aligned)\n\n# Eliminar las filas donde la columna 'id' es null\n#data_cleaned = data.filter(data[\"id\"].isNotNull())\n\ndata_cleaned = data.dropna()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\n# Limpieza de datos: Columnas principales\n#cleaned_data = data_cleaned.select(\"id\", \"title\", \"summary\", \"news_site\", \"published_at\") \\\n                  # .withColumn(\"published_date\", to_date(\"published_at\"))\ncleaned_data = data_cleaned\n#medoto para eliminar si alguna fila esta duplicada en el conjunto de datos \ncleaned_data = cleaned_data.dropDuplicates()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Escribir datos S3\noutput_path = \"s3://spaceflightbuckes2025/Silver /\"\n\n#crear archivo para la fact \n# Guardar el archivo Parquet con el mismo nombre, sobrescribiendo siempre el archivo\ncleaned_data.coalesce(1).write.mode(\"overwrite\").parquet(output_path + \"/cleaned_data\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Realizar commit del job al final\njob.commit()",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}